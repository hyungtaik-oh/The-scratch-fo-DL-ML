{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f04f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a700e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# gpu 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b4617",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af5e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the mnist dataset using keras\n",
    "data_train, data_test = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95dc11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse images and labels\n",
    "(images_train, labels_train) = data_train\n",
    "(images_test, labels_test) = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d856e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(images_train, -1)\n",
    "y = labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744341ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(tf.keras.utils.Sequence) :\n",
    "    def __init__(self, x, y, batch_size) :\n",
    "        super().__init__()\n",
    "        x = x.astype('float32')\n",
    "        y = y.astype('float32')\n",
    "        \n",
    "        self.x = [x[i:i+batch_size] for i in range(0, len(x), batch_size)]\n",
    "        self.y = [y[i:i+batch_size] for i in range(0, len(y), batch_size)]\n",
    "#         self.x = np.array_split(x, len(x) // batch_size)\n",
    "#         self.y = np.array_split(y, len(y) // batch_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42138468",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(X, y, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c74cd0",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18a10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dcd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924cce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_cnn_module(Model):\n",
    "    def __init__(self, input_channel, output_channel, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_channel = output_channel\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        \n",
    "        self.weight = tf.random.uniform([output_channel, *self.kernel_size, input_channel])\n",
    "        self.bias = tf.random.uniform([output_channel])\n",
    "        \n",
    "        self.pad_size = kernel_size // 2\n",
    "        \n",
    "        self.n_samples = None\n",
    "        self.width = None\n",
    "        self.height = None\n",
    "        \n",
    "        self.pad = padding = [[0, 0], [1, 1], [1, 1], [0, 0]]\n",
    "        \n",
    "    def _add_pad(self, img):\n",
    "        n_samples, width, height, channel = img.shape\n",
    "        self.n_samples, self.width, self.height = n_samples, width, height\n",
    "        return tf.pad(img, self.pad)\n",
    "\n",
    "    def _slicing(self, img):\n",
    "        n_samples, width, height, channel = img.shape\n",
    "\n",
    "        # product는 뒤부터 작동\n",
    "        stack = [img[:,\n",
    "                    w:w+self.kernel_size[0],\n",
    "                    h:h+self.kernel_size[0],\n",
    "                    :] for w, h in product(range(width - self.kernel_size[0] + 1), \n",
    "                                    range(width - self.kernel_size[0] + 1))\n",
    "                ]\n",
    "\n",
    "        return tf.stack(stack, -1 )\n",
    "\n",
    "    def _weight_product(self, features) :\n",
    "        stack = []\n",
    "        for n in range(self.output_channel) : # 8 -> self.output_channel\n",
    "            w = tf.expand_dims(tf.expand_dims(self.weight[n], axis=0), axis = -1)\n",
    "            stack.append(tf.math.multiply(w, features))\n",
    "            \n",
    "        return tf.reshape(tf.reduce_sum(tf.stack(stack, -1), [1,2,3]), \n",
    "                          [self.n_samples, self.width, self.height, self.output_channel]\n",
    "                         ) + self.bias\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        pad_img = self._add_pad(img)\n",
    "        sliced_img = self._slicing(pad_img)\n",
    "        feature = self._weight_product(sliced_img)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b85f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Model):\n",
    "    def __init__(self, channel_lists, kernel_size, n_class = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channel_lists = channel_lists\n",
    "            \n",
    "        layers = [tf_cnn_module(i, o, kernel_size) for i, o in zip(channel_lists, channel_lists[1:])]\n",
    "        self.cnn_layers = layers\n",
    "        \n",
    "        batch = [tf.keras.layers.BatchNormalization() for _ in channel_lists[1:]]\n",
    "        self.batch_layers = batch\n",
    "        \n",
    "        self.output_layers = tf.keras.layers.Dense(n_class)\n",
    "        self.a = tf.keras.layers.ReLU()\n",
    "        self.d = tf.keras.layers.Dropout(0.3)\n",
    "    def __call__(self, img) : \n",
    "        x  = img\n",
    "        for l, b in zip(self.cnn_layers, self.batch_layers) :\n",
    "            x = tf.transpose(l(x), perm = [0,3,1,2])\n",
    "            x = tf.transpose(b(x), perm = [0,2,3,1])\n",
    "            x = self.a(x)\n",
    "            x = self.d(x)\n",
    "\n",
    "        x = tf.math.reduce_max(tf.reshape(x, [img.shape[0], -1, self.channel_lists[-1]]), 1)\n",
    "\n",
    "#         return self.output_layers(x)\n",
    "#         return self.softmax(self.output_layers(x))\n",
    "        return tf.nn.softmax(self.output_layers(x), -1)\n",
    "    \n",
    "    def layer_maps(self, img):\n",
    "        maps = []\n",
    "        x = img\n",
    "        for l,b in zip(self.cnn_layers, self.batch_layers):\n",
    "            x = tf.transpose(l(x), perm = [0,3,1,2])\n",
    "            maps.append(x)\n",
    "            x = tf.transpose(b(x), perm = [0,2,3,1])\n",
    "            x = self.d(x)\n",
    "        return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e4a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(\n",
    "    [1, 8, 16, 32], 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba16832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_trainer:\n",
    "    def __init__(self, model) :\n",
    "        self.model = model\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "#         self.loss_fn = tf.nn.softmax_cross_entropy_with_logits()\n",
    "    def __training_batch_step__(self, batch) :\n",
    "        x, y = batch\n",
    "        with tf.GradientTape() as t:\n",
    "            y = np.eye(10)[y.astype(int)]\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.loss_fn(y, y_hat)\n",
    "        grad = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def __training_epoch__(self, epoch_numb) :\n",
    "        loss_lists = []\n",
    "        TQ = tqdm(dataset)\n",
    "        \n",
    "        for n, batch in enumerate(TQ, 1):\n",
    "            loss_lists.append(self.__training_batch_step__(batch))\n",
    "            TQ.set_description_str(f'Epoch : {epoch_numb}')\n",
    "            TQ.set_postfix_str(f'Loss : {sum(loss_lists) / n:.5}')\n",
    "        return sum(loss_lists) / n\n",
    "    \n",
    "    def fit(self, loop_numb) :\n",
    "        history = dict(\n",
    "        loss = []\n",
    "        )\n",
    "        \n",
    "        for n in range(loop_numb):\n",
    "            history['loss'].append(\n",
    "                self.__training_epoch__(n)\n",
    "            )\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c801a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tf_trainer(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3248fcbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 0: 100%|██████████| 469/469 [05:32<00:00,  1.41it/s, Loss : 14.518]\n",
      "Epoch : 1: 100%|██████████| 469/469 [05:28<00:00,  1.43it/s, Loss : 14.518]\n",
      "Epoch : 2: 100%|██████████| 469/469 [05:28<00:00,  1.43it/s, Loss : 14.518]\n",
      "Epoch : 3: 100%|██████████| 469/469 [05:28<00:00,  1.43it/s, Loss : 14.518]\n",
      "Epoch : 4: 100%|██████████| 469/469 [05:29<00:00,  1.42it/s, Loss : 14.518]\n",
      "Epoch : 5: 100%|██████████| 469/469 [05:31<00:00,  1.42it/s, Loss : 14.518]\n",
      "Epoch : 6: 100%|██████████| 469/469 [05:30<00:00,  1.42it/s, Loss : 14.518]\n",
      "Epoch : 7: 100%|██████████| 469/469 [05:33<00:00,  1.41it/s, Loss : 14.518]\n",
      "Epoch : 8: 100%|██████████| 469/469 [05:31<00:00,  1.41it/s, Loss : 14.518]\n",
      "Epoch : 9: 100%|██████████| 469/469 [05:30<00:00,  1.42it/s, Loss : 14.518]\n"
     ]
    }
   ],
   "source": [
    "history = trainer.fit(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
